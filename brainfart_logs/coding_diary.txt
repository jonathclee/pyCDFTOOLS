
## design choices

* by default the subroutines take into account of data being given as a sub-set
  -- load subset then compute, rather than compute all then subset
     -- probably less demanding on compute time and memory
     -- needs more code and checks of variable locations

* the co-ordinate files are inherited and NOT renamed
  -- could do e.g. "gdept_1d -> gdep", but code is then messy
     -- might be a good thing to force the user to think about which location
        things should be anyway
     -- can easily be done outside or as an extra post-processing routine if
        need be, and only really affects plotting routines etc.

* masks are called IN THE SUBROUTINES, rather than asking for masked data as
  input
  -- more robust, but code more unwieldy, and assumes mask exists
  -- if not, cleaner code, but places responsibility on user
     -- doesn't work for some routines (e.g., zonal mean where the length is
        needed; could ask for masked length as input I suppose)
  
* coding standard NOT conforming to NEMO
  -- that's for FORTRAN so there is no strong reason for the Python 
     implementation here to do that?
     -- probably don't even call it pyCDFTOOLS at some point, and change the 
        naming of the routines
        -- some of the algorithm details will be different (e.g. cdfmocsig does 
        not average the tracers on to V points, though it probably should be as 
        remarked in CDFTOOLS by J-M Molines [JMM])

## 17 Apr 2025

* putting bits of cdfsigmamoc together
  -- computation of a mean isoypcnal depth a bit janky, to do better

## 13 Oct 2024

* probably get rid of the e.g. "gdept_1d -> gdep" and just sort that out during
  the plotting stage

* attempting to write a vertical co-ordinate transform
  -- using the xgcm "grid.transform" function
  -- need to recreate a grid and have "outer"
     -- split out a "coord" variable, to be modified within subroutine only when 
        these kind of routines are called
     -- recreated "coord" to have consistency with NEMO convention
        -- "x_f" and "y_f" are to the RIGHT of "x_c" and "y_c"
           (since e.g. "x_f" > "x_c" for given index)
        -- "z_f" because of the plus sign is to the LEFT of "z_c" 
           (since "z_f" < " z_c" for a given index)
  -- judicious ds.transpose commands to force (t, z, y, x) ordering
  
* add in queries for the indices within the subroutines for the masks based on
  the input DataArray, so to avoid computing everything by default

## 12 Oct 2024

* renamed the imbued variables for convenienice purposes
  -- e.g. gdept_1d -> gdep
  -- e.g. gphiv    -> gphi
     -- if we do
     
        da.rename(whatever)
        
        then it does the renaming but doesn't save the new object; need instead

        da = da.rename(whatever)

* did some dimension checks to have the right dimenion sums for zonalmean

## 11 Oct 2024

* somewhat ugly way needed (?) to write zonalmean
  -- currently brute force find the grid variable
  -- would be much cleaner if the location attribute was imbued into the loaded
     variable at the loading time, then can just read it off rather than do a
     search
  ?? doesn't quite handle number of dimenions properly yet

* probably want to imbue the following onto the resulting dataset objects 
  REGARDLESS of whether they exist or not
  -- co-ordinate files
  -- time
  -- description + attributes (including "standard_name" at a minimum)

* seems to have some clashing issues currently if there is a mix in time freq
  files (e.g. "10y_grid_T" and "1y_surf_T" in the same ds object)
  -- probably best to separate the two of them for now
     -- uses slightly different loading commands
  -- to avoid a "t_bound" clash, best to just pass a ***per timeframe***
     assuming that the provided "ds" file has one frame per file (ugly, TO TIDY)

## 25 Nov 2023

! created an "in_out" module for saving variables etc.
  ?? probably want to turn everything into a class and imbue instead as
     "ds.out_netcdf(varname=blah, filename=blah)" or whatever

* probably want to imbue the following onto the resulting dataset objects if
  they don't exist by default
  -- co-ordinate files
  -- time
  -- description + attributes (including "standard_name" at a minimum)

* so far only going to have a few routines (probably curl, div, moc, eke) to
  test what things to pass and imbue onto the the resulting dataset object
  !? going to be writing subroutines rather than as a class with imbued
     functions, to leave flexibility for module namings
     -- otherwise will have to detect variable names, not flexible maybe
     -- but relies on the user to know what to throw in

* not sure how structure is going to work
  -- not pythonic to have one function per file (somewhat as in CDFTOOLS), so
     probably something like
     -- analysis.py   (the calculation based subroutines)
        -- not sure about this, because the file will probably be huge
     -- eos.py        (for equation of state things)
     -- preprocess.py (for bullying data into a form xnemogcm understands)
     -- plot.py       (generic plotting commands)
     -- cosmetics.py  (cosmetic things, e.g. add in a clock)
  -- so would do
  
     from pyCDFTOOLS.analysis import *
     moc = cdfmoc(whatever)

## 21 Nov 2023

* creation of the log
